# -*- coding: utf-8 -*-
"""OptimizaciónIISeminario.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Rsl7Mhqjw0ee29CvlzXr73KRCmqkLFY
"""

import numpy as np
import sympy as sp
import sympy.physics.vector as spv
import scipy 
import random
from sympy.plotting import plot3d
import math
from scipy.optimize import minimize
import math

"""#Punto 1

"""

def f1(x,y):
  return 8*x**2+6*y**2+4*(x-1)*(y-2)

def gradiente(fx):
  x = sp.Symbol('x')
  y = sp.Symbol('y')
  dx = fx.diff(x)
  dy = fx.diff(y)
  gradiente = []
  grad_=[]
  gradiente.append(dx)
  gradiente.append(dy)
  gradiente = sp.lambdify([x,y], gradiente)
  return gradiente

def hessiana(fx):
  x = sp.Symbol('x')
  y = sp.Symbol('y')
  dx = fx.diff(x)
  dy = fx.diff(y)
  dxx = dx.diff(x)
  dyx = dx.diff(y)
  dxy = dy.diff(x)
  dyy = dy.diff(y)
  hessiana = np.zeros((2,2))
  hessiana[0,0] = dxx
  hessiana[0,1] = dxy
  hessiana[1,0] = dxy
  hessiana[1,1] = dyy
  return hessiana

def newton(tolerancia, xn, f, i):
  grad = gradiente(f)
  hessian = hessiana(f)
  invhess = np.linalg.inv(hessian)
  normgrad = np.linalg.norm(grad(xn[0], xn[1]))
  if normgrad < tolerancia or i==10: 
    print(xn, 'es el punto mínimo en la iteración', (i))
    print('Y', f.subs([(x,xn[0]),(y,xn[1])]).evalf(), 'es el valor de la función objetivo en el punto mínimo')
  else:
    xn = xn - np.matmul(grad(xn[0], xn[1]), invhess)
    print(xn, "es una aproximación en la iteración", (i))
    return newton(tolerancia,xn, f, i+1)

x0 = []
x0.append(0.5)
x0.append(1.5)
x = sp.Symbol('x')
y = sp.Symbol('y')
newton(0.5*10**(-20),x0, f1(x,y),0)

def grad_descendiente(tolerancia, xn, f, i):
  grad = gradiente(f)

  paso_n = sp.Symbol('paso_n')
  f1 = f.subs(x,xn[0]-paso_n*grad(xn[0],xn[1])[0]).subs(y,xn[1]-paso_n*grad(xn[0],xn[1])[1])
  gradff = f1.diff(paso_n) 
  paso = sp.solve(gradff, paso_n)

  normgrad = math.sqrt(np.dot(grad(xn[0], xn[1]), grad(xn[0], xn[1])))
  if normgrad < tolerancia or i==10:
    print(xn, 'es el punto mínimo en la iteración', (i))
    print('Y', f.subs([(x,xn[0]),(y,xn[1])]).evalf(), 'es el valor de la función objetivo en el punto mínimo')
  else:
    xn = xn - np.multiply(paso,grad(xn[0], xn[1]))
    print(xn, "es una aproximación en la iteración", (i))
    return grad_descendiente(tolerancia, xn, f, i+1)

x0 = []
x0.append(0.5)
x0.append(1.5)
grad_descendiente(0.5*10**(-20),x0, f1(x,y), 0)

"""#Punto 2"""

def f2(x,y):
  return 5*x**2+5*y**2-x*y-11*x+11*y+11

x0 = []
x0.append(0)
x0.append(0)
x = sp.Symbol('x')
y = sp.Symbol('y')
newton(0.5*10**(-20),x0, f2(x,y),0)
plot3d(f2(x,y))
w, v = np.linalg.eig(hessiana(f2(x,y)))
print("Los autovalores o eigenvalues de la Hessiana de f son:", w)

def reglaArmijo(f,t,xn):
  epsilon = 0.05
  grad = gradiente(f)
  hessian = hessiana(f)
  invhess = np.linalg.inv(hessian)
  d = - np.matmul(grad(xn[0], xn[1]), invhess)
  if f.subs(x, xn[0]+t*d[0]).subs(y, xn[1]+t*d[1]) <= (f.subs(x,xn[0]).subs(y, xn[1])+epsilon*t*np.matmul(np.array(grad(xn[0],xn[1])).T,d)):
    return t
  else:
    t = t*0.5
    return reglaArmijo(f,t,xn)

def grad_descendiente(tolerancia, xn, f, i):
  grad = gradiente(f)
  normgrad = np.linalg.norm(grad(xn[0], xn[1]))
  if normgrad < tolerancia or i==10:
    print(xn, 'es el punto mínimo en la iteración', (i))
    print('Y', f.subs([(x,xn[0]),(y,xn[1])]).evalf(), 'es el valor de la función objetivo en el punto mínimo')
  else:
    paso = reglaArmijo(f, 0.1, xn)
    xn = xn - np.multiply(paso,grad(xn[0], xn[1]))
    print(xn, 'es la aproximación en la iteración', (i))
    return grad_descendiente(tolerancia, xn, f, i+1)

x0 = []
x0.append(0.5)
x0.append(1.5)
grad_descendiente(0.5*10**(-20),x0, f2(x,y), 0)

"""#Punto 3

"""

def f3(x,y,z):
  return 9-8*x-6*y-4*z+2*x**2+2*y**2+z**2+2*x*y+2*x*z

def g(x,y,z):
  return x+y+2*z-3

def gradiente2(fx):
  x = sp.Symbol('x')
  y = sp.Symbol('y')
  z = sp.Symbol('z')
  dx = fx.diff(x)
  dy = fx.diff(y)
  dz = fx.diff(z)
  gradiente2 = []
  gradiente2.append(dx)
  gradiente2.append(dy)
  gradiente2.append(dz)
  gradiente2 = sp.lambdify([x,y,z], gradiente2)
  return gradiente2

def BFGS(tolerancia, xn, fx, i,  Bk1): #SIN CONTAR LAS RESTRICCIONES
  grad = gradiente2(fx)
  normgrad = math.sqrt(np.dot(grad(xn[0], xn[1], xn[2]), grad(xn[0], xn[1], xn[2])))
  if normgrad < tolerancia:
    print(xn, 'es el punto mínimo en la iteración', (i))
  else:
    pk = -(1)*np.matmul(grad(xn[0], xn[1], xn[2]),Bk1)
    paso_n = sp.Symbol('paso_n')
    f = fx.subs(x,xn[0]+paso_n*pk[0]).subs(y,xn[1]+paso_n*pk[1]).subs(z,xn[2]+paso_n*pk[2])
    gradff = f.diff(paso_n) 
    paso = sp.solve(gradff, paso_n)
    sk = paso*pk
    xk1 = xn + sk
    yk = np.array(grad(xk1[0], xk1[1], xk1[2])) - np.array(grad(xn[0],xn[1],xn[2]))
    skyk = np.matmul(sk,yk.T)
    yksk = np.matmul(yk.T,sk)
    sksk = np.matmul(sk,sk.T)
    yksk2 = np.matmul(yk,sk.T)
    Bk2 = Bk1 + (np.identity(3)-(skyk/yksk))*Bk1*(np.identity(3)-yksk2/yksk)+sksk/yksk 
    return BFGS(tolerancia,xk1, fx, i+1, Bk2)

  
x0 = []
x0.append(0.5)
x0.append(1.5)
x0.append(1)
x = sp.Symbol('x')
y = sp.Symbol('y')
z = sp.Symbol('z')
hessinicial = np.identity(3)
BFGS(0.5*10**(-5),x0, f3(x,y,z), 0, hessinicial)

def penalizacion_interior(xn, f,g, tolerancia):
  miu = 10
  eta = 0.05
  if -tolerancia > g.subs([(x,xn[0]),(y, xn[1]),(z,xn[2])]):
    caso = f+miu*sp.ln(g)+sp.ln(-x)+sp.ln(-y)+sp.ln(-z)
  else:
    caso = f+miu*sp.ln(tolerancia)+sp.ln(-x)+sp.ln(-y)+sp.ln(-z)
  hessinicial = np.identity(3)
  mini = BFGS(tolerancia,xn,caso,0, hessinicial)
  if np.linalg.norm(mini-xn)<tolerancia:
    print(mini, 'es el punto mínimo de este problema')
  else:
    miu=miu*eta
    penalizacion_interior(mini, f,g, tolerancia,miu)
